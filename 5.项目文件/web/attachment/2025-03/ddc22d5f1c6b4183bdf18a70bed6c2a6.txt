=============================
模型的B表示什么？
=============================
模型的B表示billion，即10亿，例如32B表320亿。所谓的几B，这个度量指的是模型参数的个数，32B就有320亿个参数。一般，模型的参数越多，这个模型的性能也就越出色(可以理解为模型越聪明)。
当然模型的性能还跟其他因素有关系，但是B常常作为主要衡量数据。


=============================
一个参数有多大？
=============================
1个参数的大小，主要由参数的数据类型来决定，参数的数据类型通常有这些：
Float32(32位浮点数)
Float16(16位浮点数)
Float64(64位浮点数)
int8(8位整数)
int4(4位整数)
我们只看数字，这里的数字表示数据类型的位（比特位）个数，1个字节占8位，因此这些类型占用的字节大小为：
Float32(32位浮点数)	4字节
Float16(16位浮点数)		2字节
Float64(64位浮点数)	8字节
int8(8位整数)		1字节
int4(4位整数)		0.5字节
无论是整数还是浮点数，我们都只看它后面表示的位个数，这影响着它这种类型能表示的数值的范围，比如int8是8位，那么它能表示的数值的范围就是2^0-1~2^8-1即0-255，而int4只能表示0-15之间的数。
数据类型所能表达的数值范围越大，就能覆盖更多样化的参数值，从而在一定程度上为模型提供更高的灵活性和表达能力，模型的性能也就越好。

一个模型的参数量越多，那么这个模型的大小也就越大。


=============================
什么是蒸馏模型？
=============================
蒸馏模型，指的是用一个体积很大、性能很强的模型，叫做教师模型。而为了在计算资源有限的情况下获得类似的性能，我们会训练一个体积较小、运行速度更快的模型，这个模型叫做学生模型。
训练的过程就是让学生模型尽量模仿教师模型的行为，也就是说，在给定同样的输入时，学生模型学习输出和教师模型尽可能接近的结果。这样一来，虽然学生模型的参数更少，但它依然能够保留大模型所学到的知识和能力，从而在实际应用中表现得很好。
简单来说，模型蒸馏就是利用大模型的智慧来“指导”小模型学习，使小模型在更低的计算成本下也能取得较好的效果。 
相对应的，蒸馏出来的模型因为参数更少，因此它的大小比老师模型更小，但并没有损失太多的精度。

distill表示蒸馏的意思，一般一个模型如果带有这个单词，那么它大概率是一个蒸馏模型。


=============================
什么是量化？
=============================
量化，通常指的是将模型中使用的高精度数据（例如32位浮点数）转换为低精度数据（如8位或更低位的整数）
这种转换可以大幅降低模型的存储需求和计算资源消耗，同时在大多数情况下只会带来轻微的精度损失。量化是模型压缩技术中的一种，与知识蒸馏、剪枝等技术常常结合使用，以便在资源受限的设备（如移动设备、嵌入式设备、个人PC）上高效部署深度学习模型。


=============================
量化&蒸馏
=============================
量化主要是在模型部署阶段通过降低数据精度来直接减少模型所需的存储空间和计算资源，这可以大幅提高推理速度，并降低内存占用，适合在资源受限的设备上运行。​
而知识蒸馏则是利用一个大型、性能较好的教师模型来指导一个较小的学生模型的训练，使得学生模型能够在体积较小的前提下获得与教师模型相近的预测性能。虽然这种方法最终也能减小模型的参数量，但它的核心在于通过训练过程提升小模型的泛化能力和准确性，而不是直接压缩模型的数值表示。​

一般量化损失的精度是非线性的，比如整数8位量化到整数4位，损失大概5%(打个比方)，而量化到整数2位就会损失20%左右，这是指数性的。

量化一般用Quantized表示，意为使……量化，简写为Q，一般见到模型上标识Q2、Q4、Q5、Q8等，表示这些模型都是量化后的模型，数字则表示量化后的精度例如Q4表示量化后的精度为4位。


=============================
GGUF格式
=============================
GGUF：简单来说就是文件格式，在大语言模型（LLM）的部署和量化领域，“GGUF”是一个新兴的模型文件格式。它通常被认为是“GGML Universal Format”的缩写，即“GGML 通用格式”。这种格式是在 llama.cpp 等项目中提出的，目的是为了更灵活、高效地存储和加载各种量化后的 LLM 模型，支持更多种类的量化方案和模型特性，同时简化模型部署流程。相比于早期的 ggml 格式，GGUF 更具通用性和扩展性，正在逐步成为 LLM 模型量化后常用的标准格式。


​=============================
chatGPT历代版本
=============================
GPT-1					2018年诞生
GPT-2					2019年诞生
GPT-3					2020年诞生
InstructGPT				2022年1月发布
ChatGPT					2022年11月30日正式发布
GPT-4					2023年3月15日推出
GPT-4o					2024年5月14日发布，o代表全能
o1推理模型及各种衍生小模型o1-mini等		2024年9月13日发布






